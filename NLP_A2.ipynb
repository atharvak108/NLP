{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Hb2L3ooV7sZ"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmmTx0SUWWGE",
        "outputId": "b0ec493e-2aa5-482a-a007-98f07043e131"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"happy\", \"movie\", \"actor\", \"love\", \"sad\"]\n",
        "similar_words = {word: model.most_similar(word) for word in words}\n",
        "\n",
        "# Display similar words\n",
        "for word, similar in similar_words.items():\n",
        "    print(f\"Words similar to '{word}':\")\n",
        "    for similar_word, similarity in similar:\n",
        "        print(f\"  {similar_word}: {similarity}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtnQ9BlYWeyF",
        "outputId": "2c5fb722-c4ad-4c4e-9d16-b65478629a3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'happy':\n",
            "  glad: 0.7408890724182129\n",
            "  pleased: 0.6632170677185059\n",
            "  ecstatic: 0.6626912355422974\n",
            "  overjoyed: 0.6599286794662476\n",
            "  thrilled: 0.6514049172401428\n",
            "  satisfied: 0.6437949538230896\n",
            "  proud: 0.636042058467865\n",
            "  delighted: 0.627237856388092\n",
            "  disappointed: 0.6269949674606323\n",
            "  excited: 0.6247665286064148\n",
            "\n",
            "Words similar to 'movie':\n",
            "  film: 0.8676770329475403\n",
            "  movies: 0.8013108372688293\n",
            "  films: 0.7363011837005615\n",
            "  moive: 0.6830360889434814\n",
            "  Movie: 0.6693680286407471\n",
            "  horror_flick: 0.6577848792076111\n",
            "  sequel: 0.6577793955802917\n",
            "  Guy_Ritchie_Revolver: 0.650975227355957\n",
            "  romantic_comedy: 0.6413198709487915\n",
            "  flick: 0.6321909427642822\n",
            "\n",
            "Words similar to 'actor':\n",
            "  actress: 0.7930010557174683\n",
            "  Actor: 0.7446156740188599\n",
            "  thesp: 0.6954971551895142\n",
            "  thespian: 0.6651668548583984\n",
            "  actors: 0.6519852876663208\n",
            "  funnyman: 0.635244607925415\n",
            "  comedian_Dom_DeLuise: 0.6245246529579163\n",
            "  entertainer: 0.6184110641479492\n",
            "  Shakespearean_actor: 0.6067742705345154\n",
            "  Oscarwinning: 0.6048368215560913\n",
            "\n",
            "Words similar to 'love':\n",
            "  loved: 0.6907791495323181\n",
            "  adore: 0.6816873550415039\n",
            "  loves: 0.661863386631012\n",
            "  passion: 0.6100708842277527\n",
            "  hate: 0.600395679473877\n",
            "  loving: 0.5886635780334473\n",
            "  Ilove: 0.5702950954437256\n",
            "  affection: 0.5664337873458862\n",
            "  undying_love: 0.5547304749488831\n",
            "  absolutely_adore: 0.5536840558052063\n",
            "\n",
            "Words similar to 'sad':\n",
            "  saddening: 0.7273085713386536\n",
            "  Sad: 0.6610826849937439\n",
            "  saddened: 0.6604382395744324\n",
            "  heartbreaking: 0.6573508381843567\n",
            "  disheartening: 0.6507317423820496\n",
            "  Meny_Friedman: 0.6487058401107788\n",
            "  parishioner_Pat_Patello: 0.6475860476493835\n",
            "  saddens_me: 0.6407119035720825\n",
            "  distressing: 0.6399092674255371\n",
            "  reminders_bobbing: 0.6357713341712952\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogies = [\n",
        "    (\"king\",\"man\",\"woman\"),\n",
        "    (\"paris\",\"france\",\"germany\"),\n",
        "    (\"big\",\"bigger\",\"small\"),\n",
        "]\n",
        "\n",
        "for a, b, c in analogies:\n",
        "    result = model.most_similar(positive=[a,c], negative =[b])\n",
        "    print(f\"{a} - {b} + {c} = {result[0][0]} (similarity: {result[0][1]})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMS2SZfBWhXu",
        "outputId": "465dfb22-a76b-489d-bcda-6f915038de1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "king - man + woman = queen (similarity: 0.7118193507194519)\n",
            "paris - france + germany = berlin (similarity: 0.48413652181625366)\n",
            "big - bigger + small = large (similarity: 0.6242177486419678)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('IMDB Dataset.csv', engine='python', on_bad_lines='skip')\n"
      ],
      "metadata": {
        "id": "_Dee_zG_Wqdj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0KAEgP1Ws-y",
        "outputId": "41f5d655-6f28-4838-ff08-d3926e41e20c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "RaXVu79VWvI8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text"
      ],
      "metadata": {
        "id": "4-84knpuWxrv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_review'] = df['review'].apply(clean_text)"
      ],
      "metadata": {
        "id": "8RNpkVKyWzrm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_reviews = [review.split() for review in df['cleaned_review']]"
      ],
      "metadata": {
        "id": "kHoAzpxsW2HH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tokenized_reviews, df['sentiment'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "py8687V_W4Z8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sg_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, workers=4, sg=1)  # Skip-gram\n",
        "cbow_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, workers=4, sg=0)  # CBOW"
      ],
      "metadata": {
        "id": "v5jOD4sXW8GJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_average_word2vec(tokens_list, model, vector_size):\n",
        "    tokens_list = [token for token in tokens_list if token in model.wv.index_to_key]\n",
        "    if not tokens_list:\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean(model.wv[tokens_list], axis=0)"
      ],
      "metadata": {
        "id": "C_bEbFKVXAC0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_sg = [get_average_word2vec(review, sg_model, 100) for review in X_train]\n",
        "X_test_sg = [get_average_word2vec(review, sg_model, 100) for review in X_test]"
      ],
      "metadata": {
        "id": "xaY6uN-DXC-_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_cbow = [get_average_word2vec(review, cbow_model, 100) for review in X_train]\n",
        "X_test_cbow = [get_average_word2vec(review, cbow_model, 100) for review in X_test]"
      ],
      "metadata": {
        "id": "Ff6YccyoXGH8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.map({'positive': 1, 'negative': 0})\n",
        "y_test = y_test.map({'positive': 1, 'negative': 0})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0fgBC49nwkUk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_sg = DecisionTreeRegressor()\n",
        "dt_sg.fit(X_train_sg, y_train)\n",
        "y_pred_sg = dt_sg.predict(X_test_sg)\n",
        "accuracy_sg = accuracy_score(y_test, y_pred_sg)\n",
        "\n",
        "dt_cbow = DecisionTreeRegressor()\n",
        "dt_cbow.fit(X_train_cbow, y_train)\n",
        "y_pred_cbow = dt_cbow.predict(X_test_cbow)\n",
        "accuracy_cbow = accuracy_score(y_test, y_pred_cbow)"
      ],
      "metadata": {
        "id": "uVlDppXbXIGY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_average_pretrained_word2vec(tokens_list, model, vector_size):\n",
        "    tokens_list = [token for token in tokens_list if token in model.key_to_index]\n",
        "    if not tokens_list:\n",
        "        return np.zeros(vector_size)\n",
        "    return np.mean([model[token] for token in tokens_list], axis=0)\n",
        "\n",
        "X_train_pretrained = [get_average_pretrained_word2vec(review, model, 300) for review in X_train]\n",
        "X_test_pretrained = [get_average_pretrained_word2vec(review, model, 300) for review in X_test]\n",
        "\n",
        "# Train logistic regression model\n",
        "dt_pretrained = DecisionTreeRegressor()\n",
        "dt_pretrained.fit(X_train_pretrained, y_train)\n",
        "y_pred_pretrained = dt_pretrained.predict(X_test_pretrained)\n",
        "accuracy_pretrained = accuracy_score(y_test, y_pred_pretrained)"
      ],
      "metadata": {
        "id": "r3eKlcx-weS6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    \"Model\": [\"Skip-gram\", \"CBOW\", \"Pretrained Word2Vec\"],\n",
        "    \"Accuracy\": [accuracy_sg, accuracy_cbow, accuracy_pretrained]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "print(metrics_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFdSmWkhxHgZ",
        "outputId": "e12070e8-b8e4-49b9-a0d3-61eb231db118"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Model  Accuracy\n",
            "0            Skip-gram  0.736355\n",
            "1                 CBOW  0.708573\n",
            "2  Pretrained Word2Vec  0.675179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pHSc4arxxVIn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}